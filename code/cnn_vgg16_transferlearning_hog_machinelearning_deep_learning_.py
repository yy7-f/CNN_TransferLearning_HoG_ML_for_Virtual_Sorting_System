# -*- coding: utf-8 -*-
"""CNN_VGG16_TransferLearning_HoG_MachineLearning_Deep_learning_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18XV9fI1EzJWldJH20S138KG9ZflxlPOD

# Import libraries
"""

import cv2
import numpy as np

# import deep learning library
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

import matplotlib.pyplot as plt
import os,sys
# import deep learning library
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Dropout, Conv2D, Activation, Flatten, MaxPool2D
from tensorflow.keras.optimizers import Adam, RMSprop

import matplotlib.pyplot as plt

from PIL import ImageFile

ImageFile.LOAD_TRUNCATED_IMAGES = True

import os

"""# **Connect to the dataset (a folder in google drive)**"""

from google.colab import drive
drive.mount('/content/drive')

CLASS_NAMES = ['Blue', 'Green','Red']
CLASS_NAMES2 = ['bolts', 'cups','scissors', 'screwdrivers']
CLASS_NAMES3 = ['apple', 'bolts', 'cups','scissors', 'screwdrivers']

data_dir = 'drive/My Drive/Jupyter/Virtual_object'
training_dir = os.path.join(data_dir, 'train')
validation_dir = os.path.join(data_dir, 'validation')
test_dir = os.path.join(data_dir, 'test')

"""# **Load data using a Keras utility**

### Creat a dataset
"""

# training_dir = os.path.join(data_dir, 'train')
# validation_dir = os.path.join(data_dir, 'validation')
train_datagen = ImageDataGenerator(rescale= 1./255, shear_range = 0.2, zoom_range= 0.2, rotation_range= 20, horizontal_flip=True)

validation_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
            training_dir,
            target_size=(100,100),
            batch_size=32,
            class_mode='categorical',
            shuffle=True
)

valid_generator = validation_datagen.flow_from_directory(
            validation_dir,
            target_size=(100,100),
            batch_size=32,
            class_mode='categorical',
            shuffle=True
)

train_generator

"""# CNN model"""

model = Sequential()

model.add(Conv2D(16, kernel_size=3, activation='relu', input_shape=(100, 100, 3)))
model.add(MaxPool2D(2,2))
model.add(Dropout(0.25))
model.add(Conv2D(32, kernel_size=3, activation='relu'))
model.add(MaxPool2D(2,2))
model.add(Dropout(0.25))
model.add(Conv2D(64, kernel_size=3, activation='relu'))
model.add(MaxPool2D(2,2))
model.add(Dropout(0.25))
model.add(Conv2D(128, kernel_size=3, activation='relu'))
model.add(MaxPool2D(2,2))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(5, activation='softmax'))

model.summary()

from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4),
              loss = 'categorical_crossentropy',
              metrics = ['acc'])

NUM_TRAIN_DATA = train_generator.n
NUM_VALID_DATA = valid_generator.n

BATCH_SIZE = 32
EPOCHS = 20


my_history = model.fit(train_generator,
          steps_per_epoch=NUM_TRAIN_DATA // BATCH_SIZE,
          epochs=EPOCHS,
          validation_data=valid_generator,
          validation_steps=5,
          verbose=2)

acc = my_history.history['acc']
val_acc = my_history.history['val_acc']
loss = my_history.history['loss']
val_loss = my_history.history['val_loss']
epochs = range(1, len(acc) + 1)

f, axes = plt.subplots(1,2,figsize=(14,4))

axes[0].plot(epochs, acc, 'bo', label='Training acc')
axes[0].plot(epochs, val_acc, 'b', label='Validation acc')
axes[0].legend()

axes[1].plot(epochs, loss, 'bo', label='Training loss')
axes[1].plot(epochs, val_loss, 'b', label='Validation loss')
axes[1].yaxis.set_label_position("right")
axes[1].legend()

plt.show()

"""# Pretrained model (import the model VGG16)

"""

from keras.applications import VGG16
Model_base = VGG16()
print(Model_base.summary())



"""# [Transfer learning] Using pretrained layers as features extractor

"""

import numpy as np
from keras.applications import VGG16


conv_base = VGG16(weights='imagenet',
                  include_top=False,
                  # input_shape=(100, 100, 3))
                  input_shape=(240, 240, 3))




datagen = ImageDataGenerator(rescale=1./255)
batch_size = 32

def extract_features(directory, sample_count):
    features = np.zeros(shape=(sample_count, 7, 7, 512))
    labels = np.zeros(shape=(sample_count,5)  )#2

    generator = ImageDataGenerator(rescale=1./255).flow_from_directory(directory,
        target_size=(240, 240),
        batch_size = batch_size,
        class_mode='categorical')

    i = 0


    for inputs_batch, labels_batch in generator:
        features_batch = conv_base.predict(inputs_batch)
        features[i * batch_size : (i + 1) * batch_size] = features_batch
        labels[i * batch_size : (i + 1) * batch_size] = labels_batch
        i += 1
        if i * batch_size >= sample_count:
            break
    return features, labels


train_features, train_labels = extract_features(training_dir, 846) #16
validation_features, validation_labels = extract_features(validation_dir, 50) #4

# reshaping the features to be input to the dense layers

# train_features = np.reshape(train_features, (16, 7 * 7 * 512)) #32
# validation_features = np.reshape(validation_features, (4, 7 * 7 * 512))#32

train_features = np.reshape(train_features, (846, 7 * 7 * 512))
validation_features = np.reshape(validation_features, (50, 7 * 7 * 512))

# Building a classifier
from keras import layers
from keras import models
from keras import optimizers
classifier_model = models.Sequential()
classifier_model.add(layers.Dense(2048, activation='relu', input_dim=7 * 7 * 512))
classifier_model.add(layers.Dropout(0.5))
classifier_model.add(layers.Dense(512, activation='relu', input_dim=7 * 7 * 512))
classifier_model.add(layers.Dropout(0.5))
classifier_model.add(layers.Dense(128, activation='relu', input_dim=7 * 7 * 512))
classifier_model.add(layers.Dropout(0.5))
classifier_model.add(layers.Dense(5, activation='softmax'))
classifier_model.summary()

classifier_model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.Adam(learning_rate=1e-3),
              metrics=['acc'])
classifier_history = classifier_model.fit(train_features, train_labels,
                    epochs=50,
                    batch_size=32,
                    validation_data=(validation_features, validation_labels))

import matplotlib.pyplot as plt
acc = classifier_history.history['acc']
val_acc = classifier_history.history['val_acc']
loss = classifier_history.history['loss']
val_loss = classifier_history.history['val_loss']
epochs = range(1, len(acc) + 1)

f, axes = plt.subplots(1,2,figsize=(14,4))

axes[0].plot(epochs, acc, 'r', label='Training acc')
axes[0].plot(epochs, val_acc, 'b', label='Validation acc')
axes[0].legend()

axes[1].plot(epochs, loss, 'r', label='Training loss')
axes[1].plot(epochs, val_loss, 'b', label='Validation loss')
axes[1].yaxis.set_label_position("right")
axes[1].legend()

plt.show()

print(val_acc[-1])

"""## additional train"""

classifier_model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.Adam(learning_rate=1e-3),
              metrics=['acc'])
classifier_history = classifier_model.fit(train_features, train_labels,
                    epochs=25,
                    batch_size=16,
                    validation_data=(validation_features, validation_labels))

import matplotlib.pyplot as plt
acc = classifier_history.history['acc']
val_acc = classifier_history.history['val_acc']
loss = classifier_history.history['loss']
val_loss = classifier_history.history['val_loss']
epochs = range(1, len(acc) + 1)

f, axes = plt.subplots(1,2,figsize=(14,4))

axes[0].plot(epochs, acc, 'bo', label='Training acc')
axes[0].plot(epochs, val_acc, 'b', label='Validation acc')
axes[0].legend()

axes[1].plot(epochs, loss, 'bo', label='Training loss')
axes[1].plot(epochs, val_loss, 'b', label='Validation loss')
axes[1].yaxis.set_label_position("right")
axes[1].legend()

plt.show()

val_acc[-1]

train_features



"""# HoG

## HoG RF
"""

import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from skimage.feature import hog
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Set the parameters
img_height = 100
img_width = 100
num_train_samples = 846
num_valid_samples = 50
batch_size = 32
# train_dir = 'train'
# valid_dir = 'valid'

# Data augmentation for the training set
train_datagen = ImageDataGenerator(rescale=1./255,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

train_generator = train_datagen.flow_from_directory(training_dir,
                                                    target_size=(img_height, img_width),
                                                    batch_size=batch_size,
                                                    class_mode='categorical')

# Data augmentation for the validation set
valid_datagen = ImageDataGenerator(rescale=1./255)

valid_generator = valid_datagen.flow_from_directory(validation_dir,
                                                    target_size=(img_height, img_width),
                                                    batch_size=batch_size,
                                                    class_mode='categorical')

# Extract features for training data
train_features = []
train_labels = []

for i in range(num_train_samples):
    # Load training images
    img_path = training_dir + '/' + train_generator.filenames[i]
    img = load_img(img_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = img.astype('uint8')

    # Extract HOG features
    hog_feature = hog(img[:,:,0], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)
    train_features.append(hog_feature)
    train_labels.append(train_generator.classes[i])

train_features = np.array(train_features)
train_labels = np.array(train_labels)

# Extract features for validation data
valid_features = []
valid_labels = []

for i in range(num_valid_samples):
    # Load validation images
    img_path = validation_dir + '/' + valid_generator.filenames[i]
    img = load_img(img_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = img.astype('uint8')

    # Extract HOG features
    hog_feature = hog(img[:,:,0], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)
    valid_features.append(hog_feature)
    valid_labels.append(valid_generator.classes[i])

valid_features = np.array(valid_features)
valid_labels = np.array(valid_labels)

# Train a random forest classifier
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(train_features, train_labels)

# Evaluate the classifier on validation data
y_pred = rf_clf.predict(valid_features)
accuracy = accuracy_score(valid_labels, y_pred)
print('Accuracy:', accuracy)

import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from skimage.feature import hog
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Set the parameters
img_height = 100
img_width = 100
num_train_samples = 846
num_valid_samples = 50
batch_size = 32
# train_dir = 'train'
# valid_dir = 'valid'

# Data augmentation for the training set
train_datagen = ImageDataGenerator(rescale=1./255,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

train_generator = train_datagen.flow_from_directory(training_dir,
                                                    target_size=(img_height, img_width),
                                                    batch_size=batch_size,
                                                    class_mode='categorical')

# Data augmentation for the validation set
valid_datagen = ImageDataGenerator(rescale=1./255)

valid_generator = valid_datagen.flow_from_directory(validation_dir,
                                                    target_size=(img_height, img_width),
                                                    batch_size=batch_size,
                                                    class_mode='categorical')

# Extract features for training data
train_features = []
train_labels = []

for i in range(num_train_samples):
    # Load training images
    img_path = training_dir + '/' + train_generator.filenames[i]
    img = load_img(img_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = img.astype('uint8')

    # Extract HOG features
    hog_feature = hog(img[:,:,0], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)
    train_features.append(hog_feature)
    train_labels.append(train_generator.classes[i])

train_features = np.array(train_features)
train_labels = np.array(train_labels)

# Extract features for validation data
valid_features = []
valid_labels = []

for i in range(num_valid_samples):
    # Load validation images
    img_path = validation_dir + '/' + valid_generator.filenames[i]
    img = load_img(img_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = img.astype('uint8')

    # Extract HOG features
    hog_feature = hog(img[:,:,0], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)
    valid_features.append(hog_feature)
    valid_labels.append(valid_generator.classes[i])

valid_features = np.array(valid_features)
valid_labels = np.array(valid_labels)

# Train a random forest classifier
rf_clf = RandomForestClassifier(n_estimators=50, random_state=42)
rf_clf.fit(train_features, train_labels)

# Evaluate the classifier on validation data
y_pred = rf_clf.predict(valid_features)
accuracy = accuracy_score(valid_labels, y_pred)
print('Accuracy:', accuracy)

"""## HoG decision tree"""

import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from skimage.feature import hog
from sklearn.tree import  DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Set the parameters
img_height = 100
img_width = 100
num_train_samples = 846
num_valid_samples = 50
batch_size = 32
# train_dir = 'train'
# valid_dir = 'valid'

# Data augmentation for the training set
train_datagen = ImageDataGenerator(rescale=1./255,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

train_generator = train_datagen.flow_from_directory(training_dir,
                                                    target_size=(img_height, img_width),
                                                    batch_size=batch_size,
                                                    class_mode='categorical')

# Data augmentation for the validation set
valid_datagen = ImageDataGenerator(rescale=1./255)

valid_generator = valid_datagen.flow_from_directory(validation_dir,
                                                    target_size=(img_height, img_width),
                                                    batch_size=batch_size,
                                                    class_mode='categorical')

# Extract features for training data
train_features = []
train_labels = []

for i in range(num_train_samples):
    # Load training images
    img_path = training_dir + '/' + train_generator.filenames[i]
    img = load_img(img_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = img.astype('uint8')

    # Extract HOG features
    hog_feature = hog(img[:,:,0], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)
    train_features.append(hog_feature)
    train_labels.append(train_generator.classes[i])

train_features = np.array(train_features)
train_labels = np.array(train_labels)

# Extract features for validation data
valid_features = []
valid_labels = []

for i in range(num_valid_samples):
    # Load validation images
    img_path = validation_dir + '/' + valid_generator.filenames[i]
    img = load_img(img_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = img.astype('uint8')

    # Extract HOG features
    hog_feature = hog(img[:,:,0], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)
    valid_features.append(hog_feature)
    valid_labels.append(valid_generator.classes[i])

valid_features = np.array(valid_features)
valid_labels = np.array(valid_labels)

# Train a random forest classifier
rf_clf = DecisionTreeClassifier(random_state=42)
rf_clf.fit(train_features, train_labels)

# Evaluate the classifier on validation data
y_pred = rf_clf.predict(valid_features)
accuracy = accuracy_score(valid_labels, y_pred)
print('Accuracy:', accuracy)

"""## HoG knn"""

import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from skimage.feature import hog
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Set the parameters
img_height = 100
img_width = 100
num_train_samples = 846
num_valid_samples = 50
batch_size = 32
# train_dir = 'train'
# valid_dir = 'valid'

# Data augmentation for the training set
train_datagen = ImageDataGenerator(rescale=1./255,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

train_generator = train_datagen.flow_from_directory(training_dir,
                                                    target_size=(img_height, img_width),
                                                    batch_size=batch_size,
                                                    class_mode='categorical')

# Data augmentation for the validation set
valid_datagen = ImageDataGenerator(rescale=1./255)

valid_generator = valid_datagen.flow_from_directory(validation_dir,
                                                    target_size=(img_height, img_width),
                                                    batch_size=batch_size,
                                                    class_mode='categorical')

# Extract features for training data
train_features = []
train_labels = []

for i in range(num_train_samples):
    # Load training images
    img_path = training_dir + '/' + train_generator.filenames[i]
    img = load_img(img_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = img.astype('uint8')

    # Extract HOG features
    hog_feature = hog(img[:,:,0], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)
    train_features.append(hog_feature)
    train_labels.append(train_generator.classes[i])

train_features = np.array(train_features)
train_labels = np.array(train_labels)

# Extract features for validation data
valid_features = []
valid_labels = []

for i in range(num_valid_samples):
    # Load validation images
    img_path = validation_dir + '/' + valid_generator.filenames[i]
    img = load_img(img_path, target_size=(img_height, img_width))
    img = img_to_array(img)
    img = img.astype('uint8')

    # Extract HOG features
    hog_feature = hog(img[:,:,0], orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=False)
    valid_features.append(hog_feature)
    valid_labels.append(valid_generator.classes[i])

valid_features = np.array(valid_features)
valid_labels = np.array(valid_labels)

# Train a random forest classifier
rf_clf = KNeighborsClassifier ()
rf_clf.fit(train_features, train_labels)

# Evaluate the classifier on validation data
y_pred = rf_clf.predict(valid_features)
accuracy = accuracy_score(valid_labels, y_pred)
print('Accuracy:', accuracy)



"""# check"""

train_generator.filenames[1:3]

train_generator.class_indices

items = next(iter(train_generator))

plt.figure(figsize=(12,12))
for i, image in enumerate(items[0][:25], 1):
    plt.subplot(5,5,i)
    plt.imshow(image)
    plt.axis('off')



